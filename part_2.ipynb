{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chetan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 03:24:50.731366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "    \n",
    "## Task Specific Libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 510342192 bytes\n",
      "Memory footprint: 486.70 MB\n",
      "Memory footprint: 0.48 GB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_org = GPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\"\n",
    ")\n",
    "\n",
    "memory_footprint_bytes = model_org.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quantization 8-bit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 176527896 bytes\n",
      "Memory footprint: 168.35 MB\n",
      "Memory footprint: 0.16 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\", \n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "memory_footprint_bytes = model_8bit.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quantization 4-Bit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 134060568 bytes\n",
      "Memory footprint: 127.85 MB\n",
      "Memory footprint: 0.12 GB\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\", \n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "memory_footprint_bytes = model_4bit.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NF-4 Quantization Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 134060568 bytes\n",
      "Memory footprint: 127.85 MB\n",
      "Memory footprint: 0.12 GB\n"
     ]
    }
   ],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model_nf4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\", \n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "memory_footprint_bytes = model_nf4bit.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Corpus Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_corpus(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        corpus = []\n",
    "        for line in file:\n",
    "            # print(line.strip().lower())\n",
    "            corpus.append(line.lower())\n",
    "        return corpus\n",
    "    \n",
    "def remove_punctuation(tokenized_sentence):\n",
    "    return [word for word in tokenized_sentence if word not in string.punctuation]\n",
    "    \n",
    "# corpus_train = retrieve_corpus(\"./Dataset/ptb.train.txt\")\n",
    "# corpus_test = retrieve_corpus(\"./Dataset/ptb.test.txt\")\n",
    "# corpus_val = retrieve_corpus(\"./Dataset/ptb.valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data size: 3761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data = retrieve_corpus(\"./Dataset/ptb.test.txt\")\n",
    "test_data = [remove_punctuation(word_tokenize(sentence)) for sentence in test_data]\n",
    "\n",
    "test_size = int(1 * len(test_data))\n",
    "test_data = test_data[:test_size]\n",
    "\n",
    "print(f\"Testing data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage(device=0):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # in MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / 1024 / 1024  # in MB\n",
    "        return allocated_memory, reserved_memory\n",
    "    else:\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024 / 1024  # Memory in MB\n",
    "\n",
    "def clear_memory(device=0):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset, max_length=40, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for example in dataset:\n",
    "        inputs = tokenizer(example, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "            if( not math.isnan(loss.item())):\n",
    "                total_loss += loss.item() * inputs['input_ids'].size(1)\n",
    "                total_tokens += inputs['input_ids'].size(1)\n",
    "    \n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\n",
    "    return perplexity.item()\n",
    "\n",
    "def measure_latency(model, tokenizer, dataset, num_trials=10, max_length=40, device='cpu'):\n",
    "    model.eval()\n",
    "    latencies = []\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        example = dataset[0]  \n",
    "        inputs = tokenizer(example, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            model(**inputs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        latencies.append(end_time - start_time)\n",
    "    \n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    return avg_latency\n",
    "\n",
    "def evaluate_model(model,model_name=\"gpt2\", dataset=None, device='cpu',quantized=False):\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding token\n",
    "    if quantized is False:\n",
    "        model.to(device) \n",
    "    \n",
    "    memory_before = memory_usage()\n",
    "\n",
    "    perplexity_before = compute_perplexity(model, tokenizer, dataset, device=device)\n",
    "\n",
    "    latency_before = measure_latency(model, tokenizer, dataset, device=device)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"Memory Usage\": memory_before,\n",
    "        \"Latency\": latency_before,\n",
    "        \"Perplexity\": perplexity_before,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model\n",
      "{'Memory Usage': (931.9189453125, 984.0), 'Latency': 0.002531099319458008, 'Perplexity': 960.97509765625}\n",
      "8 Bit Quantized Model\n",
      "{'Memory Usage': (940.0439453125, 984.0), 'Latency': 0.017486882209777833, 'Perplexity': 972.1326904296875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chetan/anaconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Bit Quantized Model\n",
      "{'Memory Usage': (939.0693359375, 988.0), 'Latency': 0.007877540588378907, 'Perplexity': 1537.63427734375}\n",
      "NF4 Quantized Model\n",
      "{'Memory Usage': (939.0693359375, 988.0), 'Latency': 0.007396888732910156, 'Perplexity': 1472.7154541015625}\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(model_org,model_name=\"gpt2\", dataset=test_data, device=device)\n",
    "print(\"Original Model\")\n",
    "print(results)\n",
    "clear_memory(device)\n",
    "\n",
    "results = evaluate_model(model_8bit,model_name=\"gpt2\", dataset=test_data, device=device, quantized=True)\n",
    "print(\"8 Bit Quantized Model\")\n",
    "print(results)\n",
    "clear_memory(device)\n",
    "\n",
    "results = evaluate_model(model_4bit,model_name=\"gpt2\", dataset=test_data, device=device, quantized=True)\n",
    "print(\"4 Bit Quantized Model\")\n",
    "print(results)\n",
    "clear_memory(device)\n",
    "\n",
    "results = evaluate_model(model_nf4bit,model_name=\"gpt2\", dataset=test_data, device=device, quantized=True)\n",
    "print(\"NF4 Quantized Model\")\n",
    "print(results)\n",
    "clear_memory(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
