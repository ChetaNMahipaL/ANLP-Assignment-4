{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chetan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 12:01:38.773715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "    \n",
    "## Task Specific Libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 510342192 bytes\n",
      "Memory footprint: 486.70 MB\n",
      "Memory footprint: 0.48 GB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "model_org = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\"\n",
    ")\n",
    "\n",
    "memory_footprint_bytes = model_org.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quantization 8-bit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 176527896 bytes\n",
      "Memory footprint: 168.35 MB\n",
      "Memory footprint: 0.16 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=128.0\n",
    "    )\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\", \n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "memory_footprint_bytes = model_8bit.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quantization 4-Bit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 134060568 bytes\n",
      "Memory footprint: 127.85 MB\n",
      "Memory footprint: 0.12 GB\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\", \n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "memory_footprint_bytes = model_4bit.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NF-4 Quantization Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 134060568 bytes\n",
      "Memory footprint: 127.85 MB\n",
      "Memory footprint: 0.12 GB\n"
     ]
    }
   ],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model_nf4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\", \n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "memory_footprint_bytes = model_nf4bit.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Corpus Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_corpus(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        corpus = []\n",
    "        for line in file:\n",
    "            # print(line.strip().lower())\n",
    "            corpus.append(line.lower())\n",
    "        return corpus\n",
    "    \n",
    "def remove_punctuation(tokenized_sentence):\n",
    "    return [word for word in tokenized_sentence if word not in string.punctuation]\n",
    "    \n",
    "# corpus_train = retrieve_corpus(\"./Dataset/ptb.train.txt\")\n",
    "# corpus_test = retrieve_corpus(\"./Dataset/ptb.test.txt\")\n",
    "# corpus_val = retrieve_corpus(\"./Dataset/ptb.valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2891 samples for evaluation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test_data = retrieve_corpus(\"./Dataset/ptb.test.txt\")\n",
    "# test_data = [remove_punctuation(word_tokenize(sentence)) for sentence in test_data]\n",
    "\n",
    "# test_size = int(1 * len(test_data))\n",
    "# test_data = test_data[:test_size]\n",
    "\n",
    "# print(f\"Testing data size: {len(test_data)}\")\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "eval_texts = []\n",
    "for item in dataset:\n",
    "    if item['text'].strip():\n",
    "        eval_texts.append(item['text'])\n",
    "        if len(eval_texts) >= 3000:\n",
    "            break\n",
    "print(f\"Loaded {len(eval_texts)} samples for evaluation\")\n",
    "test_data = eval_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage(device=0):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # in MB\n",
    "        reserved_memory = torch.cuda.max_memory_allocated(device) / 1024 / 1024  # in MB\n",
    "        memory_used = reserved_memory - allocated_memory\n",
    "        return allocated_memory, reserved_memory, memory_used\n",
    "    else:\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024 / 1024  # Memory in MB\n",
    "\n",
    "def clear_memory(device=0):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset, max_length=1024, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for example in tqdm(dataset, desc=\"Calculating perplexity\"):\n",
    "            # Tokenize and prepare inputs\n",
    "            encodings = tokenizer(example, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            \n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += attention_mask.sum().item() - 1\n",
    "\n",
    "        perplexity = math.exp(total_loss / total_tokens)\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "def measure_latency(model, tokenizer, num_trials=10, max_length=1024, device='cpu'):\n",
    "    model.eval()\n",
    "    latencies = []\n",
    "    example = \"This is a sample text to measure inference performance. \" * 10 \n",
    "    inputs = tokenizer(example, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_trials):\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = model(**inputs)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latencies.append(end_time - start_time)\n",
    "    \n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    return avg_latency\n",
    "\n",
    "def evaluate_model(model,model_name=\"gpt2\", dataset=None, device='cpu',quantized=False):\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(device)\n",
    "    if quantized is False:\n",
    "        model.to(device) \n",
    "    \n",
    "    memory_before = memory_usage()\n",
    "\n",
    "    perplexity_before = compute_perplexity(model, tokenizer, dataset, device=device)\n",
    "\n",
    "    latency_before = measure_latency(model, tokenizer, device=device)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"Memory Usage\": memory_before,\n",
    "        \"Latency\": latency_before,\n",
    "        \"Perplexity\": perplexity_before,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating perplexity: 100%|██████████| 2891/2891 [00:39<00:00, 73.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Bit Quantized Model\n",
      "{'Memory Usage': (443.5751953125, 443.5751953125, 0.0), 'Latency': 0.011621427536010743, 'Perplexity': 49.854691724482834}\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(model_org,model_name=\"gpt2\", dataset=test_data, device=device)\n",
    "print(\"Original Model\")\n",
    "print(results)\n",
    "clear_memory(device)\n",
    "\n",
    "results = evaluate_model(model_8bit,model_name=\"gpt2\", dataset=test_data, device=device, quantized=True)\n",
    "print(\"8 Bit Quantized Model\")\n",
    "print(results)\n",
    "clear_memory(device)\n",
    "\n",
    "results = evaluate_model(model_4bit,model_name=\"gpt2\", dataset=test_data, device=device, quantized=True)\n",
    "print(\"4 Bit Quantized Model\")\n",
    "print(results)\n",
    "clear_memory(device)\n",
    "\n",
    "results = evaluate_model(model_nf4bit,model_name=\"gpt2\", dataset=test_data, device=device, quantized=True)\n",
    "print(\"NF4 Quantized Model\")\n",
    "print(results)\n",
    "clear_memory(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
