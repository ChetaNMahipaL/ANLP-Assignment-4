{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 20:08:01.014541: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/chetan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import gc\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Printing few Info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 510342192 bytes\n",
      "Memory footprint: 486.70 MB\n",
      "Memory footprint: 0.48 GB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "\n",
    "model_org = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\"\n",
    ")\n",
    "\n",
    "memory_footprint_bytes = model_org.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import GPT2Model\n",
    "\n",
    "# # Load the GPT-2 model\n",
    "# model_new = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# # Function to quantize weights to 8-bit\n",
    "# def quantize_weights(module):\n",
    "#     if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "#         # Quantize weights\n",
    "#         module.weight.data = torch.quantize_per_tensor(module.weight.data, scale=0.1, zero_point=0, dtype=torch.qint8)\n",
    "#         if module.bias is not None:\n",
    "#             module.bias.data = torch.quantize_per_tensor(module.bias.data, scale=0.1, zero_point=0, dtype=torch.qint8)\n",
    "\n",
    "# # Apply quantization to the model\n",
    "# model_new.apply(quantize_weights)\n",
    "\n",
    "# # Verify the quantization\n",
    "# print(model_new)\n",
    "\n",
    "# memory_footprint_bytes = model_new.get_memory_footprint()\n",
    "\n",
    "# memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "# memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "# print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "# print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "# print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quantized Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int8QuantModel(\n",
      "  (model): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2SdpaAttention(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=3072, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=3072)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Int8LinearLayer()\n",
      "  )\n",
      ")\n",
      "Memory footprint: 497759232 bytes\n",
      "Memory footprint: 474.70 MB\n",
      "Memory footprint: 0.46 GB\n"
     ]
    }
   ],
   "source": [
    "def quantize8bit(tenosrs):\n",
    "    qmin = tenosrs.min().item()\n",
    "    qmax = tenosrs.max().item()\n",
    "\n",
    "    n=8\n",
    "    scale = (qmax - qmin) / (2. ** n - 1.)\n",
    "    if scale == 0:\n",
    "        scale = 1e-8\n",
    "    zero_point = round(-1*(2**n)-qmin / scale)\n",
    "    zero_point = int(zero_point)\n",
    "\n",
    "    quant_tensor =torch.round(tenosrs / scale + zero_point).to(torch.int8)\n",
    "    return quant_tensor, scale, zero_point\n",
    "\n",
    "def dequantize8bit(quant_tensor, scale, zero_point):\n",
    "    return scale * (quant_tensor.to(torch.float32) - zero_point)\n",
    "\n",
    "def quantize_int8(tensor):\n",
    "    \"\"\"\n",
    "    Quantize tensor to INT8 with proper scaling\n",
    "    \"\"\"\n",
    "    tensor = tensor.to(torch.float32)\n",
    "    \n",
    "    qmin = tensor.min()\n",
    "    qmax = tensor.max()\n",
    "    \n",
    "    scale = (qmax - qmin) / 255.0  # 255 = 2^8 - 1\n",
    "    zero_point = (-128 - qmin / scale).round().clamp(-128, 127).to(torch.int8)\n",
    "    \n",
    "    quant_tensor = (tensor / scale + zero_point).round().clamp(-128, 127).to(torch.int8)\n",
    "    \n",
    "    return quant_tensor, scale, zero_point\n",
    "\n",
    "def dequantize_int8(quant_tensor, scale, zero_point):\n",
    "    \"\"\"\n",
    "    Dequantize INT8 tensor back to floating point\n",
    "    \"\"\"\n",
    "    return scale * (quant_tensor.to(torch.float32) - zero_point)\n",
    "\n",
    "class Int8LinearLayer(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.register_buffer(\"quant_weights\", \n",
    "                           torch.zeros((output_features, input_features), dtype=torch.int8))\n",
    "        self.register_buffer(\"scales\", \n",
    "                           torch.ones(output_features, dtype=torch.float32))\n",
    "        self.register_buffer(\"zero_points\",\n",
    "                           torch.zeros(output_features, dtype=torch.int8))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                               torch.zeros(output_features, dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass with INT8 quantized weights\n",
    "        \"\"\"\n",
    "        inputs = inputs.to(torch.float16)\n",
    "        \n",
    "        # Dequantize weights for computation\n",
    "        dequantized_weights = (\n",
    "            (self.quant_weights.float() - self.zero_points.unsqueeze(1)) * \n",
    "            self.scales.unsqueeze(1)\n",
    "        ).to(inputs.dtype)\n",
    "        \n",
    "        output = F.linear(inputs, dequantized_weights)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.to(inputs.dtype)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def quantize(self, weights):\n",
    "        \"\"\"\n",
    "        Quantize weights to INT8 with proper scaling\n",
    "        \"\"\"\n",
    "        w_fp32 = weights.detach().clone().to(torch.float32)\n",
    "        \n",
    "        for idx in range(w_fp32.size(0)):\n",
    "            row = w_fp32[idx]\n",
    "            quant_row, scale, zero_point = quantize_int8(row)\n",
    "            \n",
    "            self.quant_weights[idx] = quant_row\n",
    "            self.scales[idx] = scale\n",
    "            self.zero_points[idx] = zero_point\n",
    "\n",
    "class Int8QuantModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.exclude_layers = ['wte', 'wpe']\n",
    "        \n",
    "        self._replace_linear_layers()\n",
    "    \n",
    "    def _replace_linear_layers(self):\n",
    "        \"\"\"\n",
    "        Replace standard linear layers with INT8 quantized layers\n",
    "        \"\"\"\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Linear) and not any(exclude in name for exclude in self.exclude_layers):\n",
    "                parent = self._get_parent_module(name)\n",
    "                if parent is not None:\n",
    "                    layer_name = name.split('.')[-1]\n",
    "                    new_layer = Int8LinearLayer(\n",
    "                        module.in_features,\n",
    "                        module.out_features,\n",
    "                        bias=module.bias is not None,\n",
    "                        dtype=module.weight.dtype\n",
    "                    )\n",
    "                    new_layer.quantize(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        new_layer.bias = module.bias\n",
    "                    \n",
    "                    setattr(parent, layer_name, new_layer)\n",
    "    \n",
    "    def _get_parent_module(self, name):\n",
    "        \"\"\"\n",
    "        Get parent module for a given module name\n",
    "        \"\"\"\n",
    "        parent_path = '.'.join(name.split('.')[:-1])\n",
    "        if not parent_path:\n",
    "            return self.model\n",
    "            \n",
    "        try:\n",
    "            parent = self.model\n",
    "            for part in parent_path.split('.'):\n",
    "                parent = getattr(parent, part)\n",
    "            return parent\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        if input_ids is not None:\n",
    "            input_ids = input_ids.long()\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(torch.float16)\n",
    "            \n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def get_memory_footprint(self):\n",
    "        \"\"\"\n",
    "        Calculate total memory footprint of the quantized model using numel().\n",
    "        Returns total memory usage in bytes.\n",
    "        \"\"\"\n",
    "        total_bytes = 0\n",
    "        \n",
    "        # For each named parameter in the model\n",
    "        for name, param in self.model.named_parameters():\n",
    "            total_bytes += param.numel()*param.element_size()\n",
    "                \n",
    "        return total_bytes\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\"\n",
    ")\n",
    "# Quantize all linear layers in the GPT-2 model\n",
    "model_quant=Int8QuantModel(model)\n",
    "print(model_quant)\n",
    "\n",
    "memory_footprint_bytes = model_quant.get_memory_footprint()\n",
    "\n",
    "memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "memory_footprint_gb = memory_footprint_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory footprint: {memory_footprint_bytes} bytes\")\n",
    "print(f\"Memory footprint: {memory_footprint_mb:.2f} MB\")\n",
    "print(f\"Memory footprint: {memory_footprint_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Partial Quantization Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Decoder_QuantizedModel' object has no attribute 'original_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh)\n\u001b[1;32m     85\u001b[0m quantized_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 86\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m Decoder_QuantizedModel(model, quantized_blocks)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(quantized_model\u001b[38;5;241m.\u001b[39mmodel_size)\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mDecoder_QuantizedModel.__init__\u001b[0;34m(self, original_model, quantized_blocks)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantized_blocks \u001b[38;5;241m=\u001b[39m quantized_blocks\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(original_model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Decoder_QuantizedModel' object has no attribute 'original_model'"
     ]
    }
   ],
   "source": [
    "def quantize_to_int8(tensor):\n",
    "    \"\"\"Quantize tensor to int8 with proper scaling\"\"\"\n",
    "    tensor = tensor.detach().float()\n",
    "    qmin, qmax = -128, 127\n",
    "    min_val, max_val = tensor.min().item(), tensor.max().item()\n",
    "    \n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    scale = max(scale, 1e-8)\n",
    "    zero_point = qmin - round(min_val / scale)\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = torch.round(tensor / scale + zero_point).clamp(qmin, qmax).to(torch.int8)\n",
    "    return quantized, scale, zero_point\n",
    "\n",
    "def dequantize_from_int8(quantized_tensor, scale, zero_point):\n",
    "    \"\"\"Dequantize int8 tensor back to floating point\"\"\"\n",
    "    return (quantized_tensor.float() - zero_point) * scale\n",
    "\n",
    "class Decoder_QuantizedModel(torch.nn.Module):\n",
    "    def __init__(self, original_model, quantized_blocks):\n",
    "        super().__init__()\n",
    "        self.quantization_params = {}\n",
    "        self.model_size = 0\n",
    "        self.quantized_blocks = quantized_blocks\n",
    "        \n",
    "        self.device = next(original_model.parameters()).device\n",
    "        \n",
    "        self.original_model = self.original_model.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for name, param in self.original_model.named_parameters():\n",
    "                param.data = param.data.to(self.device) \n",
    "                is_decoder_block = any(f\"transformer.h.{idx}.\" in name for idx in self.quantized_blocks)\n",
    "                if is_decoder_block:\n",
    "                    quantized_tensor, scale, zero_point = quantize_to_int8(param.data)\n",
    "                    self.model_size += quantized_tensor.numel()\n",
    "                    \n",
    "                    self.quantization_params[name] = {\n",
    "                        \"quantized\": quantized_tensor.to(self.device),\n",
    "                        \"scale\": scale,\n",
    "                        \"zero_point\": zero_point,\n",
    "                    }\n",
    "                    \n",
    "                    dequantized = dequantize_from_int8(\n",
    "                        quantized_tensor, \n",
    "                        scale, \n",
    "                        zero_point\n",
    "                    ).to(self.device)\n",
    "                    \n",
    "                    param.data.copy_(dequantized)\n",
    "                else:\n",
    "                    self.model_size += param.numel() * param.element_size()\n",
    "        \n",
    "        self.model_size = self.model_size / (1024 ** 2)\n",
    "    \n",
    "    def to(self, device):\n",
    "        \"\"\"Override to method to handle device movement\"\"\"\n",
    "        self.device = device\n",
    "        self.original_model = self.original_model.to(device)\n",
    "        return super().to(device)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "        if labels is not None:\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = self.original_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                **kwargs\n",
    "            )\n",
    "        return outputs\n",
    "\n",
    "# Usage example\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Quantize middle layers\n",
    "num_layers = len(model.transformer.h)\n",
    "quantized_blocks = list(range(1, num_layers-1))\n",
    "quantized_model = Decoder_QuantizedModel(model, quantized_blocks)\n",
    "print(quantized_model.model_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Corpus Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_corpus(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        corpus = []\n",
    "        for line in file:\n",
    "            # print(line.strip().lower())\n",
    "            corpus.append(line.lower())\n",
    "        return corpus\n",
    "    \n",
    "def remove_punctuation(tokenized_sentence):\n",
    "    return [word for word in tokenized_sentence if word not in string.punctuation]\n",
    "    \n",
    "# corpus_train = retrieve_corpus(\"./Dataset/ptb.train.txt\")\n",
    "# corpus_test = retrieve_corpus(\"./Dataset/ptb.test.txt\")\n",
    "# corpus_val = retrieve_corpus(\"./Dataset/ptb.valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2891 samples for evaluation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test_data = retrieve_corpus(\"./Dataset/ptb.test.txt\")\n",
    "# test_data = [remove_punctuation(word_tokenize(sentence)) for sentence in test_data]\n",
    "\n",
    "# test_size = int(1 * len(test_data))\n",
    "# test_data = test_data[:test_size]\n",
    "\n",
    "# print(f\"Testing data size: {len(test_data)}\")\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "eval_texts = []\n",
    "for item in dataset:\n",
    "    if item['text'].strip():\n",
    "        eval_texts.append(item['text'])\n",
    "        if len(eval_texts) >= 3000:\n",
    "            break\n",
    "print(f\"Loaded {len(eval_texts)} samples for evaluation\")\n",
    "test_data = eval_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage(device=0):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # in MB\n",
    "        reserved_memory = torch.cuda.max_memory_allocated(device) / 1024 / 1024  # in MB\n",
    "        memory_used = reserved_memory - allocated_memory\n",
    "        return allocated_memory, reserved_memory, memory_used\n",
    "    else:\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024 / 1024  # Memory in MB\n",
    "\n",
    "def clear_memory(device=0):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset, max_length=1024, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for example in tqdm(dataset, desc=\"Calculating perplexity\"):\n",
    "            # Tokenize and prepare inputs\n",
    "            encodings = tokenizer(example, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            \n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += attention_mask.sum().item() - 1\n",
    "\n",
    "        perplexity = math.exp(total_loss / total_tokens)\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "def measure_latency(model, tokenizer, num_trials=10, max_length=1024, device='cpu'):\n",
    "    model.eval()\n",
    "    latencies = []\n",
    "    example = \"This is a sample text to measure inference performance. \" * 10 \n",
    "    inputs = tokenizer(example, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_trials):\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = model(**inputs)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latencies.append(end_time - start_time)\n",
    "    \n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    return avg_latency\n",
    "\n",
    "def evaluate_model(model,model_name=\"gpt2\", dataset=None, device='cpu'):\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(device)\n",
    "    model.to(device) \n",
    "    \n",
    "    memory_before = memory_usage()\n",
    "\n",
    "    perplexity_before = compute_perplexity(model, tokenizer, dataset, device=device)\n",
    "\n",
    "    latency_before = measure_latency(model, tokenizer, device=device)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"Memory Usage\": memory_before,\n",
    "        \"Latency\": latency_before,\n",
    "        \"Perplexity\": perplexity_before,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating perplexity: 100%|██████████| 2891/2891 [00:39<00:00, 72.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Bit Partial Quantized Model\n",
      "{'Memory Usage': (1995.67626953125, 1995.67626953125, 0.0), 'Latency': 0.011525678634643554, 'Perplexity': 51.41608254277834}\n"
     ]
    }
   ],
   "source": [
    "# results = evaluate_model(model_org, model_name=\"gpt2\", dataset=test_data, device=device)\n",
    "# print(\"Original Model\")\n",
    "# print(results)\n",
    "# clear_memory(device)\n",
    "\n",
    "results = evaluate_model(quantized_model, model_name=\"gpt2\", dataset=test_data, device=device)\n",
    "print(\"8 Bit Partial Quantized Model\")\n",
    "print(results)\n",
    "clear_memory(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
